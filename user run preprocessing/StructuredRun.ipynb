{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured run of the model experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "These imports include tools that help with reading the data, preprocessing the data, creating the predictive models and metrics for those models. It also includes interpolation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_sensor_files(location):\n",
    "    \"\"\"\n",
    "    Read processed sensor files.\n",
    "\n",
    "    Parameters:\n",
    "    location (str): Location of the directory containing the files.\n",
    "\n",
    "    Returns:\n",
    "    list: List of DataFrames read from the files.\n",
    "    \"\"\"\n",
    "\n",
    "    # List files in directory and sort them\n",
    "    files = [file for file in os.listdir(location)]\n",
    "    files.sort()\n",
    "\n",
    "    # Read files into a list of DataFrames\n",
    "    read_files = []\n",
    "    for file in files:\n",
    "        read_files.append(pd.read_csv(location + '/' + file))\n",
    "        #read_files.append(pd.read_csv(os.path.join(location, file)))  \n",
    "\n",
    "    # Extract columns from each DataFrame\n",
    "    cols_file = [read_files[i].columns.tolist() for i in range(len(read_files))]\n",
    "\n",
    "    # Find common elements across all DataFrames' columns\n",
    "    common_els = reduce(set.intersection, (set(item) for item in cols_file))\n",
    "    common_els = list(common_els)\n",
    "\n",
    "    # Filter columns to keep only common ones\n",
    "    read_files = [read_files[i][common_els] for i in range(len(read_files))]\n",
    "\n",
    "    return read_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files = '../../Processed Sensors'\n",
    "FILES = read_processed_sensor_files(path_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Downtime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_downtime(location):\n",
    "    \"\"\"\n",
    "    Process downtime data.\n",
    "\n",
    "    Parameters:\n",
    "    downtime (DataFrame): DataFrame containing downtime data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Processed DataFrame with the following columns:\n",
    "        - 'FaultDateTime': Combined fault date and time as datetime.\n",
    "        - 'Duration': Duration of the fault.\n",
    "        - 'Equipment': Equipment type.\n",
    "        - Other columns with relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read downtime data\n",
    "    downtime = pd.read_excel(location)\n",
    "\n",
    "    # Convert 'FaultDate' to datetime and extract date\n",
    "    downtime['FaultDate'] = pd.to_datetime(downtime['FaultDate'], errors='coerce').dt.date\n",
    "\n",
    "    # Format 'FaultTime' \n",
    "    downtime['FaultTime'] = downtime['FaultTime'].apply(lambda x: x.strftime('%H:%M:%S') if len(str(x)) > 8 else x)\n",
    "\n",
    "    # Drop rows with missing 'FaultDate' or 'FaultTime'\n",
    "    downtime = downtime.dropna(subset=['FaultDate', 'FaultTime'])\n",
    "\n",
    "    # Select relevant columns and drop unnecessary ones\n",
    "    pattern_new = downtime.iloc[:, :14]\n",
    "    pattern_new = pattern_new.drop(['ID', 'DutyOfficer', 'Manager email address'], axis=1)\n",
    "\n",
    "    # Combine 'FaultDate' and 'FaultTime' into 'FaultDateTime'\n",
    "    pattern_new['FaultDateTime'] = pd.to_datetime(pattern_new['FaultDate'].astype(str) + ' ' + pattern_new['FaultTime'].astype(str))\n",
    "\n",
    "    # Drop 'FaultDate' and 'FaultTime' columns\n",
    "    pattern_new.drop(['FaultDate', 'FaultTime'], axis=1, inplace=True)\n",
    "\n",
    "    # Sort by 'FaultDateTime'\n",
    "    pattern_new.sort_values(by=['FaultDateTime'], inplace=True)\n",
    "\n",
    "    # Drop additional unnecessary columns\n",
    "    pattern_new = pattern_new.drop(['LogEntry', 'DutyOfficer comments', 'Managerscomments', 'FaultRepair', 'FaultDescription', 'Group', 'Downtime'], axis=1)\n",
    "\n",
    "    # Convert 'Equipment' to lowercase and remove special characters\n",
    "    pattern_new['Equipment'] = pattern_new['Equipment'].str.lower()\n",
    "    pattern_new['Equipment'] = pattern_new['Equipment'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Filter rows where 'Equipment' is 'ion source'\n",
    "    pattern_new = pattern_new[pattern_new['Equipment'] == 'ion source']\n",
    "\n",
    "    return pattern_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling process downtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_downtime = '../../Raw Data/Equipment downtime data (202310).xlsx'\n",
    "DOWNTIME = process_downtime(path_downtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing user run and intiializing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_runs(user_runs, read_files, pattern_new):\n",
    "    \"\"\"\n",
    "    Process user runs and label sensor data.\n",
    "\n",
    "    Parameters:\n",
    "    user_runs (list): List of user runs.\n",
    "    read_files (list): List of DataFrames containing sensor data.\n",
    "    pattern_new (DataFrame): DataFrame containing processed fault pattern data.\n",
    "\n",
    "    Returns:\n",
    "    list: List of DataFrames with labeled sensor data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate list of DataFrames for each user run\n",
    "    isrc_fail_runs = [pattern_new[pattern_new['User Run'] == ur] for ur in user_runs]\n",
    "\n",
    "    # Iterate over each DataFrame in read_files\n",
    "    for i in range(len(read_files)):\n",
    "        # Initialize 'Label' column with 0\n",
    "        read_files[i]['Label'] = 0\n",
    "        # Convert 'Time' column to datetime\n",
    "        read_files[i]['Time'] = pd.to_datetime(read_files[i]['Time'], format='%d/%m/%Y, %H:%M:%S')\n",
    "        # Iterate over rows\n",
    "        for j in range(read_files[i].shape[0]-1):\n",
    "            # Check if any fault occurred during the time interval\n",
    "            for el in isrc_fail_runs[i]['FaultDateTime']:\n",
    "                if read_files[i]['Time'][j] <= el < read_files[i]['Time'][j+1]:\n",
    "                    read_files[i].loc[j, 'Label'] = 1\n",
    "                    break\n",
    "\n",
    "    return read_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling process user runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "userruns = ['18/04', '19/01', '19/02', '19/03', '20/01', '20/02', '20/03', '21/01', '21/02', '22/01', '22/02', '22/03', '22/04', '22/05', '23/01', '23/02', '23/03']\n",
    "PROCESSED_FILES = process_user_runs(userruns, FILES, DOWNTIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions with varying results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time window creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_time_window(read_files, twindow):\n",
    "    \"\"\"\n",
    "    Apply a time window to labeled sensor data.\n",
    "\n",
    "    Parameters:\n",
    "    read_files (list): List of DataFrames containing sensor data with labels.\n",
    "    twindow (int): Time window size.\n",
    "\n",
    "    Returns:\n",
    "    list: List of DataFrames with labels adjusted according to the time window.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each DataFrame in read_files\n",
    "    for i in range(len(read_files)):\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for j in range(read_files[i].shape[0]):\n",
    "            # If the label is 1\n",
    "            if read_files[i]['Label'][j] == 1:\n",
    "                # Update labels within the time window\n",
    "                for k in range(twindow):\n",
    "                    if j - k - 1 < 0:\n",
    "                        break\n",
    "                    read_files[i].loc[j - k - 1, 'Label'] = 1\n",
    "\n",
    "    return read_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensors to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_SENSORS = [\n",
    "    'Time',\n",
    "    'irfq::mag:read_volts',\n",
    "    'irfq::gas:back_press',\n",
    "    'irfq::mag:read_current',\n",
    "    'irfq::arc_dc:read_current',\n",
    "    'irfq::arc_pulser_on:sta',\n",
    "    'irfq::gas:read_press',\n",
    "    'irfq::ion_source:sta',\n",
    "    'irfq::arc_dc:set_current',\n",
    "    'irfq::h2_gas:read_flow',\n",
    "    'irfq::arc_ac:read_current',\n",
    "    'irfq::gas:on',\n",
    "    'irfq::arc_remote:sta',\n",
    "    'irfq::arc_dc_on:sta',\n",
    "    'irfq::arc_dc:on',\n",
    "    'irfq::arc_dc:read_volts',\n",
    "    'irfq::platform:read_volts',\n",
    "    'irfq::isrce_line:read_press',\n",
    "    'irfq::ext:set_volts',\n",
    "    'irfq::mag_on:sta',\n",
    "    'irfq::arc_ac:set_current',\n",
    "    'irfq::ext:read_current',\n",
    "    'irfq::arc_pulser_intlk:sta',\n",
    "    'irfq::ext:read_volts',\n",
    "    'irfq::gas:set_press',\n",
    "    'Label'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sensor_data(rfNew, kbSensors):\n",
    "    \"\"\"\n",
    "    Process sensor data for training and testing.\n",
    "\n",
    "    Parameters:\n",
    "    rfNew (list): List of DataFrames containing sensor data.\n",
    "    kbSensors (list): List of columns to keep in the sensor data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing X_train, X_test, y_train, and y_test DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select columns to keep in sensor data\n",
    "    rfNew = [rfNew[i][kbSensors] for i in range(len(rfNew))]\n",
    "\n",
    "    # Concatenate DataFrames for training and testing\n",
    "    X_train = pd.concat(rfNew[:14])\n",
    "    X_test = pd.concat(rfNew[14:])\n",
    "\n",
    "    # Extract labels\n",
    "    y_train = X_train['Label']\n",
    "    y_test = X_test['Label']\n",
    "\n",
    "    # Drop 'Label' and 'Time' columns\n",
    "    X_train.drop(['Label', 'Time'], axis=1, inplace=True)\n",
    "    X_test.drop(['Label', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_FOREST = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10, \n",
    "    random_state=42,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    criterion='entropy',\n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBooost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST = XGBClassifier(n_estimators=100, max_depth=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_CLASSIFIER = SVC(kernel='rbf', random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGISTIC_REGRESSION = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Smoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTER = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing(model, twindow):\n",
    "    \"\"\"\n",
    "    Perform testing with and without SMOTE.\n",
    "\n",
    "    This function applies a time window to processed sensor data, splits it into training and testing sets,\n",
    "    fits a given model to the training data, and evaluates its performance on the testing data using confusion matrix\n",
    "    and classification report. Additionally, it applies SMOTE (Synthetic Minority Over-sampling Technique) to the\n",
    "    training data, fits the model to the resampled data, and evaluates its performance on the testing data.\n",
    "\n",
    "    Parameters:\n",
    "    model: A machine learning model object with fit and predict methods.\n",
    "    twindow (int): Time window size for label adjustment.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing confusion matrix and classification report for both the model without SMOTE\n",
    "    (confusion matrix, classification report) and with SMOTE (confusion matrix with SMOTE, classification report with SMOTE).\n",
    "    \"\"\"\n",
    "    labelized_files = apply_time_window(PROCESSED_FILES, twindow)\n",
    "    X_train, X_test, y_train, y_test = process_sensor_data(labelized_files, KB_SENSORS)\n",
    "\n",
    "    # Train model without SMOTE\n",
    "    model_unsmoted = model\n",
    "    model_unsmoted.fit(X_train, y_train)\n",
    "    y_pred = model_unsmoted.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Apply SMOTE\n",
    "    X_train_smote, y_train_smote = SMOTER.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train model with SMOTE\n",
    "    model_smoted = model\n",
    "    model_smoted.fit(X_train_smote, y_train_smote)\n",
    "    y_pred_smote = model_smoted.predict(X_test)\n",
    "    cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "    cr_smote = classification_report(y_test, y_pred_smote)\n",
    "\n",
    "    return cm, cr, cm_smote, cr_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWINDOWS = [10, 20, 30, 40, 50, 60, 120, 180, 360, 720, 1440]\n",
    "MODELS = [RANDOM_FOREST]#, XGBOOST, SV_CLASSIFIER, LOGISTIC_REGRESSION, KNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RandomForestClassifier(criterion='entropy', max_depth=10, n_jobs=-1,\n",
      "                       random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time window:  10\n",
      "Confusion matrix without SMOTE:\n",
      "[[145113      0]\n",
      " [   330      0]]\n",
      "Classification report without SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    145113\n",
      "           1       0.00      0.00      0.00       330\n",
      "\n",
      "    accuracy                           1.00    145443\n",
      "   macro avg       0.50      0.50      0.50    145443\n",
      "weighted avg       1.00      1.00      1.00    145443\n",
      "\n",
      "Confusion matrix with SMOTE:\n",
      "[[144192    921]\n",
      " [   327      3]]\n",
      "Classification report with SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    145113\n",
      "           1       0.00      0.01      0.00       330\n",
      "\n",
      "    accuracy                           0.99    145443\n",
      "   macro avg       0.50      0.50      0.50    145443\n",
      "weighted avg       1.00      0.99      0.99    145443\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m twindow \u001b[38;5;129;01min\u001b[39;00m TWINDOWS:\n\u001b[0;32m----> 8\u001b[0m     cm, cr, cm_smote, cr_smote \u001b[38;5;241m=\u001b[39m \u001b[43mTesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime window: \u001b[39m\u001b[38;5;124m'\u001b[39m, twindow)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion matrix without SMOTE:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[77], line 33\u001b[0m, in \u001b[0;36mTesting\u001b[0;34m(model, twindow)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train model with SMOTE\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model_smoted \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel_smoted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_smote\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m y_pred_smote \u001b[38;5;241m=\u001b[39m model_smoted\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     35\u001b[0m cm_smote \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred_smote)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cms = []\n",
    "crs = []\n",
    "cms_smote = []\n",
    "crs_smote = []\n",
    "for model in MODELS:\n",
    "    print('Model: ', model)\n",
    "    for twindow in TWINDOWS:\n",
    "        cm, cr, cm_smote, cr_smote = Testing(model, twindow)\n",
    "        print('Time window: ', twindow)\n",
    "        print('Confusion matrix without SMOTE:')\n",
    "        print(cm)\n",
    "        print('Classification report without SMOTE:')\n",
    "        print(cr)\n",
    "        print('Confusion matrix with SMOTE:')\n",
    "        print(cm_smote)\n",
    "        print('Classification report with SMOTE:')\n",
    "        print(cr_smote)\n",
    "        print('\\n')\n",
    "        cms.append(cm)\n",
    "        crs.append(cr)\n",
    "        cms_smote.append(cm_smote)\n",
    "        crs_smote.append(cr_smote)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
